{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/08 11:00:33 WARN Utils: Your hostname, Dominique resolves to a loopback address: 127.0.1.1; using 172.22.70.171 instead (on interface eth0)\n",
      "22/04/08 11:00:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/08 11:00:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName='challenge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = ss.read.csv('./node_information.csv') # ici, on lit un fichier qui peut Ãªtre sur le cluster (auquel cas il faut mettre l'adresse du cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id='1001', year='2000', topics='compactification geometry and duality', authors='Paul S. Aspinwall', empty=None, abstract='these are notes based on lectures given at tasi99 we review the geometry of the moduli space of n 2 theories in four dimensions from the point of view of superstring compactification the cases of a type iia or type iib string compactified on a calabi-yau threefold and the heterotic string compactified on k3xt2 are each considered in detail we pay specific attention to the differences between n 2 theories and n 2 theories the moduli spaces of vector multiplets and the moduli spaces of hypermultiplets are reviewed in the case of hypermultiplets this review is limited by the poor state of our current understanding some peculiarities such as mixed instantons and the non-existence of a universal hypermultiplet are discussed')]\n"
     ]
    }
   ],
   "source": [
    "# load articles\n",
    "df_articles = dft.toDF('id', 'year', 'topics', 'authors', 'empty', 'abstract')\n",
    "print(df_articles.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "links dataframe\n",
      "[Row(source='9510123', target='9502114', class='1')]\n",
      "############################################\n"
     ]
    }
   ],
   "source": [
    "# load edges\n",
    "df_links = ss.read.csv('./training_set.txt', sep=\" \").toDF('source', 'target', 'class')\n",
    "print(\"links dataframe\")\n",
    "print(df_links.head(1))\n",
    "print(\"############################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create new column\n",
      "[Row(id='1001', year='2000', topics='compactification geometry and duality', authors='Paul S. Aspinwall', empty=None, abstract='these are notes based on lectures given at tasi99 we review the geometry of the moduli space of n 2 theories in four dimensions from the point of view of superstring compactification the cases of a type iia or type iib string compactified on a calabi-yau threefold and the heterotic string compactified on k3xt2 are each considered in detail we pay specific attention to the differences between n 2 theories and n 2 theories the moduli spaces of vector multiplets and the moduli spaces of hypermultiplets are reviewed in the case of hypermultiplets this review is limited by the poor state of our current understanding some peculiarities such as mixed instantons and the non-existence of a universal hypermultiplet are discussed', title_lower='compactification geometry and duality')]\n",
      "################\n"
     ]
    }
   ],
   "source": [
    "# Creation of a new column\n",
    "from pyspark.sql.functions import lower, col\n",
    "\n",
    "df_articles = df_articles.withColumn('title_lower', lower(col('topics')))\n",
    "print(\"create new column\")\n",
    "print(df_articles.head(1))\n",
    "print(\"################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join with python function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:===================>                                       (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id='10010', year='2000', topics='supersymmetry and f-theory realization of the deformed conifold with', authors='Steven S. Gubser (Princeton University)', empty=None, abstract='three-form flux it is shown that the deformed conifold solution with three-form flux found by klebanov and strassler is supersymmetric and that it admits a simple f-theory description in terms of a direct product of the deformed conifold and a torus some general remarks on ramond-ramond backgrounds and warped compactifications are included', title_lower='supersymmetry and f-theory realization of the deformed conifold with', source='10010', target='107263', class='0')]\n",
      "################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# do a join\n",
    "inner_join_example = df_articles.join(df_links, df_articles.id == df_links.source)\n",
    "print(\"join with python function\")\n",
    "print(inner_join_example.head(1))\n",
    "print(\"################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop a column\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id='10010', year='2000', authors='Steven S. Gubser (Princeton University)', empty=None, abstract='three-form flux it is shown that the deformed conifold solution with three-form flux found by klebanov and strassler is supersymmetric and that it admits a simple f-theory description in terms of a direct product of the deformed conifold and a torus some general remarks on ramond-ramond backgrounds and warped compactifications are included', title_lower='supersymmetry and f-theory realization of the deformed conifold with', source='10010', target='107263', class='0')]\n",
      "################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "inner_join_example = inner_join_example.drop(inner_join_example.topics)\n",
    "print(\"drop a column\")\n",
    "print(inner_join_example.head(1))\n",
    "print(\"################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple select with SQL\n",
      "[]\n",
      "################\n"
     ]
    }
   ],
   "source": [
    "#We can do both and more complex stuff with SQL as well\n",
    "#But first we need tables to refer to \n",
    "\n",
    "df_articles.createOrReplaceTempView(\"text\")\n",
    "df_links.createOrReplaceTempView(\"links\")\n",
    "print(\"simple select with SQL\")\n",
    "print(ss.sql(\"Select source,target,class from links where source like '%par%'\" ).head(1))\n",
    "print(\"################\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count classes\n",
      "+-----+--------+\n",
      "|class|count(1)|\n",
      "+-----+--------+\n",
      "|    0|  280382|\n",
      "|    1|  335130|\n",
      "+-----+--------+\n",
      "\n",
      "None\n",
      "################\n"
     ]
    }
   ],
   "source": [
    "#Count how many elements per class\n",
    "print(\"count classes\")\n",
    "print(ss.sql(\"Select class,count(*) from links group by class\" ).show())\n",
    "print(\"################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join and column selection with SQL\n",
      "[]\n",
      "################\n"
     ]
    }
   ],
   "source": [
    "#Join and select some of the columns\n",
    "inner_join_example2=ss.sql(\"\"\"Select source,target, text1.abstract as txt1,class \n",
    "\t\t\t\t\t\t\t\tfrom links \n",
    "\t\t\t\t\t\t\t\tinner join text as text1 on text1.title_lower==source\"\"\")\n",
    "print(\"join and column selection with SQL\")\n",
    "print(inner_join_example2.head(1))\n",
    "print(\"################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import lower, col\n",
    "import numpy as np\n",
    "from pyspark.ml.linalg import SparseVector, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('example').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles = spark.read.csv(\"./node_information.csv\",sep=\",\").toDF(\"ID\",\"Year\",\"Intro\",\"Authors\",\"Empty\",\"Abstract\")\n",
    "df_links = spark.read.csv(\"./training_set.txt\",sep=\" \").toDF(\"source\",\"target\",\"class\")\n",
    "df_articles = df_articles.withColumn(\"title_lower\",lower(col(\"Intro\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles.createOrReplaceTempView(\"text\")\n",
    "df_links.createOrReplaceTempView(\"links\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_join=spark.sql(\"\"\"Select source,target, text1.title_lower as txt1,class \n",
    "\t\t\t\t\t\t\t\tfrom links \n",
    "\t\t\t\t\t\t\t\tinner join text as text1 on text1.ID==source\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(source='9510123', target='9502114', txt1='an infinite number of potentials surrounding 2d black hole', class='1')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inner_join.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create features with word count on source\t\t\t\t\t\t\t\t\n",
    "def transform(row):\n",
    "\tdata=row.asDict()#Rows are immutable; We convert them to dictionaries\n",
    "\tif data['txt1']==None : data['txt1']='' #We want o avoid any null issues\n",
    "\t#get unique words\n",
    "\ttext1=set([x.lower() for x in  data['txt1'].encode('utf-8').split()])\n",
    "\t#create a new column with an array of number to be the features\n",
    "\tdata[\"features\"]=[len(text1)]#only one feature\n",
    "\t#keep only features and class\n",
    "\tret={}\n",
    "\tret[\"features\"]=data[\"features\"]\n",
    "\tret[\"class\"]=float(data[\"class\"])\n",
    "\t#convert the dictionary back to a Row\n",
    "\tnewRow = Row(*ret.keys()) #a. the Row object specification (column names)\n",
    "\tnewRow = newRow(*ret.values())#b. the corresponding column values\n",
    "\treturn newRow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataframe with map on RDD\n",
      "[Row(features=[9], class=1.0), Row(features=[8], class=1.0), Row(features=[6], class=0.0), Row(features=[9], class=0.0), Row(features=[5], class=0.0)]\n",
      "################\n"
     ]
    }
   ],
   "source": [
    "data=inner_join.rdd.map(transform).toDF()\t\n",
    "print(\"New dataframe with map on RDD\")\n",
    "print(data.head(5))\n",
    "print(\"################\")\t\n",
    "#models use sparse arrays\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets apply a function directly on the dataframe\n",
    "#UDF\n",
    "#applying functions to a DATAFRAME requires the \"SQL\" logic of \n",
    "#User Defined Functions (UDF)\n",
    "#as an example: convert features to sparse array\n",
    "#1. define what data type your UDF returns VectorUDT : UDF SCHEMA\n",
    "custom_udf_schema = VectorUDT()\n",
    "#2. define function \n",
    "def to_sparse_(v):\n",
    "\t\timport numpy as np\n",
    "\t\tif isinstance(v, SparseVector):\n",
    "\t\t\treturn v\n",
    "\t\tvs = np.array(v)\n",
    "\t\tnonzero = np.nonzero(vs)[0]\n",
    "\t\treturn SparseVector(len(v), nonzero, vs[nonzero])\n",
    "#3. create a udf from that function and the schema\n",
    "to_sparse = udf(to_sparse_,custom_udf_schema)\n",
    "#4. apply UDF to DF and create new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new sparse array column\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 63:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(features=[9], class=1.0, feats=SparseVector(1, {0: 9.0})), Row(features=[8], class=1.0, feats=SparseVector(1, {0: 8.0})), Row(features=[6], class=0.0, feats=SparseVector(1, {0: 6.0})), Row(features=[9], class=0.0, feats=SparseVector(1, {0: 9.0})), Row(features=[5], class=0.0, feats=SparseVector(1, {0: 5.0}))]\n",
      "################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data= data.withColumn('feats',to_sparse(data.features))\n",
    "\n",
    "print(\"new sparse array column\")\n",
    "print(data.head(5))\n",
    "print(\"################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(source='9510123', target='9502114', txt1='an infinite number of potentials surrounding 2d black hole', class='1'),\n",
       " Row(source='9707075', target='9604178', txt1='d 6 n 1 string vacua and duality', class='1'),\n",
       " Row(source='9312155', target='9506142', txt1='conformal field theory and hyperbolic geometry', class='0'),\n",
       " Row(source='9911255', target='302165', txt1='comparing instanton contributions with exact results in n 2', class='0'),\n",
       " Row(source='9701033', target='209076', txt1='quantum gravitational measure for three-geometries', class='0'),\n",
       " Row(source='9710020', target='9709228', txt1='n 4 4 2d supersymmetric gauge theory and brane configuration', class='1'),\n",
       " Row(source='9901042', target='9510135', txt1='k-theory reality and orientifolds', class='1'),\n",
       " Row(source='209146', target='9502077', txt1='logarithmic correlation functions in liouville field theory', class='0'),\n",
       " Row(source='9705079', target='9702201', txt1='hermitian d-brane solutions', class='1'),\n",
       " Row(source='3016', target='9207067', txt1='gravitational waves in open de sitter space', class='0')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inner_join.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "13420b7fd676705b1c4fa2323bb8774999428326742cf1ab7adeafa46a1c7521"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('dssp-iT5HcaP5-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
